
import json
import aiohttp
import asyncio
import requests
import time
from datetime import datetime
from aiohttp import ClientTimeout
import os
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", "your_token_here")

# é…ç½®é¡¹
INPUT_FILE = 'input.json'
OUTPUT_FILE = 'output.json'
FAILED_LOG_FILE = 'failed_apis.log'
GITHUB_LOG_FILE = 'github_sources.log'
MAX_GITHUB_RESULTS = 2000
MAX_RETRIES = 3
GITHUB_TOKEN = 'your_token_here'  # â† æ›¿æ¢ä¸ºä½ çš„ GitHub Token

SENSITIVE_KEYWORDS = [
    "å†…å°„", "ä¸­å‡º", "å¼ºå¥¸", "è°ƒæ•™", "ä¹±ä¼¦", "sm", "é»‘æ–™", "æ¯ç‹—", "ç²¾æ¶²", "æ— ç ", "æœ‰ç "]
DEFAULT_CATEGORIES = [
    { "name": "çƒ­é—¨", "type": "movie", "query": "çƒ­é—¨" },
    { "name": "æœ€æ–°", "type": "movie", "query": "æœ€æ–°" },
    { "name": "ç»å…¸", "type": "movie", "query": "ç»å…¸" },
    { "name": "è±†ç“£é«˜åˆ†", "type": "movie", "query": "è±†ç“£é«˜åˆ†" },
    { "name": "å†·é—¨ä½³ç‰‡", "type": "movie", "query": "å†·é—¨ä½³ç‰‡" },
    { "name": "åè¯­", "type": "movie", "query": "åè¯­" },
    { "name": "æ¬§ç¾", "type": "movie", "query": "æ¬§ç¾" },
    { "name": "éŸ©å›½", "type": "movie", "query": "éŸ©å›½" },
    { "name": "æ—¥æœ¬", "type": "movie", "query": "æ—¥æœ¬" },
    { "name": "åŠ¨ä½œ", "type": "movie", "query": "åŠ¨ä½œ" },
    { "name": "å–œå‰§", "type": "movie", "query": "å–œå‰§" },
    { "name": "çˆ±æƒ…", "type": "movie", "query": "çˆ±æƒ…" },
    { "name": "ç§‘å¹»", "type": "movie", "query": "ç§‘å¹»" },
    { "name": "æ‚¬ç–‘", "type": "movie", "query": "æ‚¬ç–‘" },
    { "name": "ææ€–", "type": "movie", "query": "ææ€–" },
    { "name": "Rçº§", "type": "movie", "query": "Rçº§" },
    { "name": "ä¸‰çº§", "type": "movie", "query": "ä¸‰çº§" },
    { "name": "æƒ…è‰²", "type": "movie", "query": "æƒ…è‰²" },
    { "name": "çƒ­é—¨", "type": "tv", "query": "çƒ­é—¨" },
    { "name": "ç¾å‰§", "type": "tv", "query": "ç¾å‰§" },
    { "name": "è‹±å‰§", "type": "tv", "query": "è‹±å‰§" },
    { "name": "éŸ©å‰§", "type": "tv", "query": "éŸ©å‰§" },
    { "name": "æ—¥å‰§", "type": "tv", "query": "æ—¥å‰§" },
    { "name": "å›½äº§å‰§", "type": "tv", "query": "å›½äº§å‰§" },
    { "name": "æ¸¯å‰§", "type": "tv", "query": "æ¸¯å‰§" },
    { "name": "æ—¥æœ¬åŠ¨ç”»", "type": "tv", "query": "æ—¥æœ¬åŠ¨ç”»" },
    { "name": "ç»¼è‰º", "type": "tv", "query": "ç»¼è‰º" },
    { "name": "çºªå½•ç‰‡", "type": "tv", "query": "çºªå½•ç‰‡" }
]
# æ—¥å¿—å‡½æ•°
def log_failed(api_url, reason):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open(FAILED_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"[{timestamp}] {api_url} - {reason}\n")

def log_github_source(url, status, extra=None):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    line = f"[{timestamp}] {status} - {url}"
    if extra:
        line += f" - {extra}"
    with open(GITHUB_LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(line + "\n")

# GitHub è¯·æ±‚å¤´
def github_headers():
    return {
        "Accept": "application/vnd.github.v3+json",
        "Authorization": f"token {GITHUB_TOKEN}"
    }

# æœ¬åœ° JSON åŠ è½½
def load_local_json():
    try:
        with open(INPUT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f).get("api_site", {})
    except Exception as e:
        print(f"âš ï¸ æ— æ³•åŠ è½½æœ¬åœ°æ–‡ä»¶ï¼š{e}")
        return {}

# GitHub æœç´¢
def search_github_configs():
    url = "https://api.github.com/search/code"
    params = {
        "q": '"cache_time": 7200 "api_site" in:file language:JSON',
        "sort": "indexed",
        "order": "desc"
    }
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = requests.get(url, headers=github_headers(), params=params, timeout=10)
            response.raise_for_status()
            return response.json().get("items", [])[:MAX_GITHUB_RESULTS]
        except Exception as e:
            print(f"âš ï¸ GitHub æœç´¢å¤±è´¥ï¼ˆå°è¯• {attempt}/{MAX_RETRIES}ï¼‰ï¼š{e}")
            time.sleep(2)
    return []

# å®‰å…¨è§£æ JSON
def safe_json_load(text, raw_url=None):
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "api_site" in data:
            return {
                "api_site": data["api_site"],
                "custom_category": data.get("custom_category", [])
            }
        else:
            log_github_source(raw_url, "âŒ éé¢„æœŸç»“æ„", f"ç±»å‹: {type(data).__name__}")
            return {"api_site": {}, "custom_category": []}
    except json.JSONDecodeError as e:
        log_github_source(raw_url, "âŒ JSONè¯­æ³•é”™è¯¯", str(e))
        if raw_url:
            debug_file = f"debug_{raw_url.split('/')[-1]}"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(text)
        return {"api_site": {}, "custom_category": []}
    except Exception as e:
        log_github_source(raw_url, "âŒ è§£æå¼‚å¸¸", str(e))
        return {"api_site": {}, "custom_category": []}

# æå– GitHub é…ç½®
def fetch_and_extract_api_sites(item):
    raw_url = item["html_url"].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
    repo = item.get("repository", {})
    author = repo.get("owner", {}).get("login", "unknown")
    updated_at = repo.get("updated_at", "unknown")
    size_kb = item.get("size", 0) / 1024

    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = requests.get(raw_url, headers=github_headers(), timeout=5)
            response.raise_for_status()
            text = response.content.decode('utf-8-sig')
            result = safe_json_load(text, raw_url)
            if result["api_site"]:
                log_github_source(
                    raw_url,
                    "âœ… è§£ææˆåŠŸ",
                    f"ä½œè€…: {author}, æ›´æ–°æ—¶é—´: {updated_at}, å¤§å°: {size_kb:.1f} KB"
                )
            return result
        except Exception as e:
            if attempt == MAX_RETRIES:
                log_github_source(
                    raw_url,
                    "âŒ è¯·æ±‚å¤±è´¥",
                    f"ä½œè€…: {author}, æ›´æ–°æ—¶é—´: {updated_at}, é”™è¯¯: {str(e)}"
                )
            time.sleep(1)
    return {"api_site": {}, "custom_category": []}

# åˆ†ç±»é¡¹åˆå¹¶å»é‡
def merge_custom_categories(all_categories):
    seen = set()
    merged = []
    for item in all_categories:
        key = (item.get("name"), item.get("type"), item.get("query"))
        if key not in seen:
            seen.add(key)
            merged.append(item)
    return merged

# API å¯ç”¨æ€§æ£€æµ‹
async def check_api(session, api_url, name):
    try:
        async with session.get(api_url, timeout=ClientTimeout(total=5)) as response:
            if response.status != 200:
                log_failed(api_url, f"Status {response.status}")
                return False
            text = await response.text()
            for keyword in SENSITIVE_KEYWORDS:
                if keyword.lower() in name.lower():
                    log_failed(api_url, f"Filtered by name: {keyword}")
                    return False
                if keyword.lower() in text.lower():
                    log_failed(api_url, f"Filtered by content: {keyword}")
                    return False
            return True
    except Exception as e:
        log_failed(api_url, str(e))
        return False

# ä¸»æµç¨‹
async def main():
    all_sites = {}
    all_categories = []

    local_sites = load_local_json()
    for site in local_sites.values():
        api_url = site.get("api")
        if api_url and api_url not in all_sites:
            all_sites[api_url] = site

    for item in search_github_configs():
        result = fetch_and_extract_api_sites(item)
        for site in result["api_site"].values():
            api_url = site.get("api")
            if api_url and api_url not in all_sites:
                all_sites[api_url] = site
        all_categories.extend(result["custom_category"])

    connector = aiohttp.TCPConnector(ssl=False)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = [check_api(session, api_url, site.get("name", "")) for api_url, site in all_sites.items()]
        results = await asyncio.gather(*tasks)

    new_api_site = {}
    index = 1
    for i, (api_url, site) in enumerate(all_sites.items()):
        if results[i]:
            new_api_site[str(index)] = site
            index += 1

    new_data = {
     "cache_time": 7200,
    "api_site": new_api_site,
    "custom_category": merge_custom_categories(DEFAULT_CATEGORIES + all_categories)
    }

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®Œæˆï¼šä¿ç•™ {len(new_api_site)} ä¸ªåˆæ³•ä¸”å¯ç”¨ API")
    print(f"ğŸ“„ é”™è¯¯æ—¥å¿—å·²ä¿å­˜åˆ° {FAILED_LOG_FILE}")
    print(f"ğŸ“„ GitHub æ—¥å¿—å·²ä¿å­˜åˆ° {GITHUB_LOG_FILE}")
    print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜ä¸º {OUTPUT_FILE}")

# è¿è¡Œ
if __name__ == '__main__':
    asyncio.run(main())
